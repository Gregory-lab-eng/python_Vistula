{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a0390adff624fad8c38b18b606cbbb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8492445d06de48eea0365f8bf32c5fb4",
              "IPY_MODEL_657979b481794e1d80d7fcff37c2f373",
              "IPY_MODEL_bb11ce6d9a6241c6826e3c27ef432f72"
            ],
            "layout": "IPY_MODEL_e5a1da4c72f5415ebb4d9c8f49396a1e"
          }
        },
        "8492445d06de48eea0365f8bf32c5fb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e05bb30113ae4daaaeef117eea556235",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8f3b355f75084770acb5165484a77784",
            "value": "Batches:‚Äá100%"
          }
        },
        "657979b481794e1d80d7fcff37c2f373": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_771492ee684e415fa597bb0f088320fd",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6ceed45e12b4106a0bcf4da6ea64a62",
            "value": 12
          }
        },
        "bb11ce6d9a6241c6826e3c27ef432f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_813087e905b44de0956d5bb202e86299",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_682b3304df784f99aeb3cf5340aae965",
            "value": "‚Äá12/12‚Äá[00:57&lt;00:00,‚Äá‚Äá3.78s/it]"
          }
        },
        "e5a1da4c72f5415ebb4d9c8f49396a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e05bb30113ae4daaaeef117eea556235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f3b355f75084770acb5165484a77784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "771492ee684e415fa597bb0f088320fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6ceed45e12b4106a0bcf4da6ea64a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "813087e905b44de0956d5bb202e86299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "682b3304df784f99aeb3cf5340aae965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEeFAzi_o7mT"
      },
      "source": [
        "# Notebook 1/2 ‚Äî Prosty system RAG (ChromaDB + LiteLLM) + UI w Streamlit\n",
        "\n",
        "**Cel ƒáwiczenia:** zbudujesz minimalny system **RAG** (Retrieval-Augmented Generation), kt√≥ry:\n",
        "1. wczytuje dokumenty (np. PDF/TXT/MD),\n",
        "2. dzieli je na fragmenty (chunking),\n",
        "3. zapisuje wektory w **ChromaDB**,\n",
        "4. pobiera najbardziej podobne fragmenty (retrieval),\n",
        "5. generuje odpowied≈∫ przez LLM (OpenAI/Claude/Gemini/OpenRouter) z u≈ºyciem **LiteLLM**,\n",
        "6. udostƒôpnia interfejs w **Streamlit**.\n",
        "\n",
        "> Daty w tym notatniku sƒÖ przyk≈Çadowe. Dzi≈õ: **2026-01-18**.\n",
        "\n",
        "---\n",
        "\n",
        "## Wymagania\n",
        "- konto i klucz API (opcjonalnie) do wybranego dostawcy LLM:\n",
        "  - OpenAI: `OPENAI_API_KEY`\n",
        "  - Anthropic Claude: `ANTHROPIC_API_KEY`\n",
        "  - Google Gemini: `GEMINI_API_KEY`\n",
        "  - OpenRouter: `OPENROUTER_API_KEY` (czasem jako `OPENAI_API_KEY` z odpowiednim base_url; w LiteLLM jest to uproszczone)\n",
        "- W tym notebooku **embeddingi** robimy lokalnie (`sentence-transformers`), ≈ºeby nie wymagaƒá p≈Çatnych API.\n",
        "\n",
        "---\n",
        "\n",
        "## Spos√≥b pracy\n",
        "1. Uruchom kom√≥rki od g√≥ry do do≈Çu.\n",
        "2. Na ko≈Ñcu uruchom aplikacjƒô Streamlit i otw√≥rz link z tunelu.\n"
      ],
      "id": "dEeFAzi_o7mT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhhVhvN-o7mU"
      },
      "source": [
        "# Instalacja bibliotek\n",
        "!pip -q install chromadb==0.5.5 sentence-transformers==3.0.1 litellm==1.44.22 streamlit==1.37.1 pypdf==4.3.1 python-dotenv==1.0.1\n",
        "\n",
        "# Narzƒôdzie do tunelowania Streamlit w Colab (cloudflared)\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i -q cloudflared-linux-amd64.deb || true\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "xhhVhvN-o7mU"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"numpy<2.0\" --force-reinstall\n"
      ],
      "metadata": {
        "id": "pwAkjCiiJ4Ff"
      },
      "id": "pwAkjCiiJ4Ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRBTs0gFo7mV"
      },
      "source": [
        "## 1) Importy i konfiguracja\n",
        "\n",
        "- Baza wektorowa: ChromaDB (persist na dysku)\n",
        "- Embedding: `sentence-transformers` (model: `all-MiniLM-L6-v2`)\n",
        "- LLM: LiteLLM (jedno API do wielu provider√≥w)\n",
        "\n",
        "> Je≈õli nie masz klucza do LLM, nadal mo≈ºesz przetestowaƒá czƒô≈õƒá retrieval (wyszukiwanie fragment√≥w).\n"
      ],
      "id": "mRBTs0gFo7mV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9ubPU-_o7mV"
      },
      "source": [
        "import os, re, json, textwrap, pathlib, time\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pypdf import PdfReader\n",
        "\n",
        "import litellm\n",
        "\n",
        "PERSIST_DIR = \"./chroma_db\"\n",
        "COLLECTION_NAME = \"docs\"\n",
        "\n",
        "# Lokalny model embedding√≥w\n",
        "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "\n",
        "# Chroma klient + kolekcja\n",
        "client = chromadb.PersistentClient(path=PERSIST_DIR, settings=Settings(anonymized_telemetry=False))\n",
        "collection = client.get_or_create_collection(name=COLLECTION_NAME)\n",
        "\n",
        "print(\"Chroma persist dir:\", os.path.abspath(PERSIST_DIR))\n",
        "print(\"Collection:\", collection.name)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "K9ubPU-_o7mV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s2ybNWao7mW"
      },
      "source": [
        "## 2) Wczytywanie dokument√≥w i chunking\n",
        "\n",
        "Wykorzystujemy prosty chunker:\n",
        "- dzieli tekst na fragmenty o rozmiarze ok. `chunk_size` znak√≥w,\n",
        "- zachowuje `overlap` dla lepszego kontekstu.\n",
        "\n",
        "Mo≈ºesz wgraƒá pliki przez `files.upload()` albo wkleiƒá tekst bezpo≈õrednio.\n"
      ],
      "id": "4s2ybNWao7mW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "641uwquco7mW"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "def read_pdf(path: str) -> str:\n",
        "    reader = PdfReader(path)\n",
        "    pages = []\n",
        "    for p in reader.pages:\n",
        "        pages.append(p.extract_text() or \"\")\n",
        "    return \"\\n\".join(pages)\n",
        "\n",
        "def read_text(path: str) -> str:\n",
        "    return pathlib.Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "def load_documents(uploaded_paths: List[str]) -> List[Dict[str, Any]]:\n",
        "    docs = []\n",
        "    for p in uploaded_paths:\n",
        "        ext = pathlib.Path(p).suffix.lower()\n",
        "        if ext == \".pdf\":\n",
        "            txt = read_pdf(p)\n",
        "        else:\n",
        "            txt = read_text(p)\n",
        "        docs.append({\"path\": p, \"text\": txt})\n",
        "    return docs\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 150) -> List[str]:\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(text):\n",
        "        chunk = text[i:i+chunk_size]\n",
        "        chunks.append(chunk)\n",
        "        i += max(1, chunk_size - overlap)\n",
        "    return chunks\n",
        "\n",
        "# Upload plik√≥w (PDF/TXT/MD)\n",
        "uploaded = files.upload()\n",
        "uploaded_paths = list(uploaded.keys())\n",
        "print(\"Uploaded:\", uploaded_paths)\n",
        "\n",
        "docs = load_documents(uploaded_paths)\n",
        "print(\"Loaded documents:\", len(docs))\n",
        "print(\"Example preview:\", docs[0][\"text\"][:400] if docs else \"(none)\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "641uwquco7mW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O30_y9PEo7mX"
      },
      "source": [
        "## 3) Indeksowanie w ChromaDB\n",
        "\n",
        "Ka≈ºdy chunk dostaje:\n",
        "- `id` (unikalny),\n",
        "- `document` (tekst),\n",
        "- `metadane` (≈∫r√≥d≈Ço + numer chunka),\n",
        "- `embedding` (wektor).\n",
        "\n",
        "> Je≈õli uruchamiasz notebook wielokrotnie, mo≈ºesz wyczy≈õciƒá kolekcjƒô.\n"
      ],
      "id": "O30_y9PEo7mX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tViNBAwxo7mX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "1a0390adff624fad8c38b18b606cbbb4",
            "8492445d06de48eea0365f8bf32c5fb4",
            "657979b481794e1d80d7fcff37c2f373",
            "bb11ce6d9a6241c6826e3c27ef432f72",
            "e5a1da4c72f5415ebb4d9c8f49396a1e",
            "e05bb30113ae4daaaeef117eea556235",
            "8f3b355f75084770acb5165484a77784",
            "771492ee684e415fa597bb0f088320fd",
            "c6ceed45e12b4106a0bcf4da6ea64a62",
            "813087e905b44de0956d5bb202e86299",
            "682b3304df784f99aeb3cf5340aae965"
          ]
        },
        "outputId": "facdfa8a-0987-480a-b063-3ae3d20b5ec5"
      },
      "source": [
        "def reset_collection():\n",
        "    global collection\n",
        "    client.delete_collection(COLLECTION_NAME)\n",
        "    collection = client.get_or_create_collection(name=COLLECTION_NAME)\n",
        "    print(\"Collection reset.\")\n",
        "\n",
        "def index_documents(docs: List[Dict[str, Any]], chunk_size=1000, overlap=150):\n",
        "    ids, texts, metas, embeds = [], [], [], []\n",
        "    for d in docs:\n",
        "        chunks = chunk_text(d[\"text\"], chunk_size=chunk_size, overlap=overlap)\n",
        "        for j, ch in enumerate(chunks):\n",
        "            uid = f\"{pathlib.Path(d['path']).name}::chunk{j}\"\n",
        "            ids.append(uid)\n",
        "            texts.append(ch)\n",
        "            metas.append({\"source\": d[\"path\"], \"chunk\": j})\n",
        "    if not texts:\n",
        "        print(\"No text to index.\")\n",
        "        return\n",
        "\n",
        "    # embedding batch\n",
        "    embeds = embedder.encode(texts, batch_size=32, show_progress_bar=True).tolist()\n",
        "    collection.add(ids=ids, documents=texts, metadatas=metas, embeddings=embeds)\n",
        "    print(f\"Indexed {len(texts)} chunks.\")\n",
        "\n",
        "# (Opcjonalnie) reset_collection()\n",
        "index_documents(docs)\n",
        "print(\"Total vectors:\", collection.count())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a0390adff624fad8c38b18b606cbbb4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexed 369 chunks.\n",
            "Total vectors: 369\n"
          ]
        }
      ],
      "id": "tViNBAwxo7mX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWJ1WfrTo7mX"
      },
      "source": [
        "## 4) Retrieval: pobieranie najlepszych fragment√≥w\n",
        "\n",
        "Funkcja `retrieve(query, k)` zwraca top-k fragment√≥w wraz z metadanymi i dystansem/podobie≈Ñstwem.\n"
      ],
      "id": "QWJ1WfrTo7mX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C1FKwq_o7mX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2f1fd5-89c0-405d-c1d3-69a8eac0f328"
      },
      "source": [
        "def retrieve(query: str, k: int = 5):\n",
        "    q_emb = embedder.encode([query]).tolist()\n",
        "    res = collection.query(query_embeddings=q_emb, n_results=k, include=[\"documents\", \"metadatas\", \"distances\"])\n",
        "    hits = []\n",
        "    for doc, meta, dist in zip(res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n",
        "        hits.append({\"text\": doc, \"meta\": meta, \"distance\": float(dist)})\n",
        "    return hits\n",
        "\n",
        "query = \"O czym jest dokument?\"\n",
        "hits = retrieve(query, k=3)\n",
        "for i,h in enumerate(hits,1):\n",
        "    print(f\"#{i} dist={h['distance']:.4f} source={h['meta']}\")\n",
        "    print(h[\"text\"][:250], \"\\n\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#1 dist=1.6801 source={'chunk': 136, 'source': 'sql-performance-explainedpdf-pdf-free.pdf'}\n",
            "ject as bind parameter. This is yet another benefit of bind parameters. If you cannot do that, you just have to convert the search term instead of the table column: SELECT ... FROM sales WHERE sale_date = TO_DATE('1970-01-01', 'YYYY-MM-DD') This quer \n",
            "\n",
            "#2 dist=1.6911 source={'chunk': 315, 'source': 'sql-performance-explainedpdf-pdf-free.pdf'}\n",
            "............................................................................ 166 Getting an Execution Plan ......................................................... 166 Operations ...................................................................... \n",
            "\n",
            "#3 dist=1.6936 source={'chunk': 8, 'source': 'sql-performance-explainedpdf-pdf-free.pdf'}\n",
            ".. 162 Update .................................................................................... 163 A. Execution Plans .......................................................................... 165 Oracle Database ................................. \n",
            "\n"
          ]
        }
      ],
      "id": "_C1FKwq_o7mX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9ruA0FXo7mY"
      },
      "source": [
        "## 5) Generowanie odpowiedzi przez LLM (LiteLLM)\n",
        "\n",
        "LiteLLM wspiera wiele provider√≥w. Ustal `model` np.:\n",
        "- OpenAI: `gpt-4o-mini`, `gpt-4.1-mini` itd.\n",
        "- Anthropic: `claude-3-5-sonnet-20240620` itd.\n",
        "- Gemini: `gemini-1.5-pro`, `gemini-1.5-flash`\n",
        "- OpenRouter: `openrouter/<nazwa_modelu>` (np. `openrouter/google/gemini-flash-1.5`)\n",
        "\n",
        "### Klucze API\n",
        "Ustaw w ≈õrodowisku Colab (na czas sesji):\n",
        "```python\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"...\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"...\"\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = \"...\"\n",
        "```\n",
        "\n",
        "> Je≈õli nie ustawisz klucza, ta czƒô≈õƒá zwr√≥ci b≈ÇƒÖd ‚Äî to OK, wtedy testuj retrieval.\n"
      ],
      "id": "J9ruA0FXo7mY"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "lNpHMYASRSVH"
      },
      "id": "lNpHMYASRSVH",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEDqohYNo7mY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5edb78e5-dd5f-485a-d06f-8ebb9aa40952"
      },
      "source": [
        "def rag_answer(question: str, k: int = 5, model: str = \"gpt-4o-mini\") -> Dict[str, Any]:\n",
        "    hits = retrieve(question, k=k)\n",
        "    context = \"\\n\\n\".join([f\"[{i+1}] ({h['meta']['source']}, chunk {h['meta']['chunk']})\\n{h['text']}\" for i,h in enumerate(hits)])\n",
        "\n",
        "    system = \"Jeste≈õ pomocnym asystentem. Odpowiadaj po polsku. Je≈õli brakuje danych w kontek≈õcie, powiedz wprost czego nie wiesz.\"\n",
        "    user = f\"\"\"Pytanie: {question}\n",
        "\n",
        "Kontekst (wybrane fragmenty dokument√≥w):\n",
        "{context}\n",
        "\n",
        "Instrukcja: Odpowiedz na pytanie wy≈ÇƒÖcznie na podstawie kontekstu. Je≈õli kontekst nie wystarcza, napisz jakie informacje sƒÖ brakujƒÖce.\n",
        "\"\"\"\n",
        "    resp = litellm.completion(\n",
        "        model=model,\n",
        "        messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    answer = resp[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return {\"answer\": answer, \"hits\": hits, \"model\": model}\n",
        "\n",
        "# Przyk≈Çad (wymaga klucza API do wybranego modelu)\n",
        "result = rag_answer(\"Jakie sƒÖ g≈Ç√≥wne tezy dokumentu?\", model=\"gpt-4o-mini\")\n",
        "print(result[\"answer\"])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Na podstawie dostƒôpnych fragment√≥w dokumentu mo≈ºna wyodrƒôbniƒá kilka g≈Ç√≥wnych tez:\n",
            "\n",
            "1. **Problemy z wydajno≈õciƒÖ SQL**: Problemy z wydajno≈õciƒÖ SQL sƒÖ powszechne, mimo ≈ºe SQL nie jest ju≈º tak wolny jak w jego poczƒÖtkowych wersjach. Wydajno≈õƒá SQL jest tematem, kt√≥ry wciƒÖ≈º wymaga uwagi.\n",
            "\n",
            "2. **Separacja \"co\" i \"jak\"**: SQL jako jƒôzyk programowania czwartej generacji (4GL) pozwala na oddzielenie opisu tego, co jest potrzebne, od sposobu, w jaki to jest realizowane. U≈ºytkownik nie musi znaƒá wewnƒôtrznych mechanizm√≥w bazy danych, aby napisaƒá zapytanie.\n",
            "\n",
            "3. **Znajomo≈õƒá dzia≈Çania bazy danych**: Wiele os√≥b, kt√≥re majƒÖ do≈õwiadczenie w SQL, nie posiada wiedzy na temat przetwarzania danych w bazie, co mo≈ºe prowadziƒá do nieefektywnego korzystania z tego jƒôzyka.\n",
            "\n",
            "Brakuje jednak szczeg√≥≈Çowych informacji na temat konkretnych rozwiƒÖza≈Ñ problem√≥w z wydajno≈õciƒÖ SQL oraz przyk≈Çad√≥w zastosowania najlepszych praktyk w kontek≈õcie optymalizacji zapyta≈Ñ.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected `CompletionTokensDetails` - serialized value may not be as expected [field_name='completion_tokens_details', input_value={'accepted_prediction_tok...d_prediction_tokens': 0}, input_type=dict])\n",
            "  PydanticSerializationUnexpectedValue(Expected `PromptTokensDetails` - serialized value may not be as expected [field_name='prompt_tokens_details', input_value={'audio_tokens': 0, 'cached_tokens': 0}, input_type=dict])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        }
      ],
      "id": "KEDqohYNo7mY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWpkyWaCo7mY"
      },
      "source": [
        "## 6) Aplikacja Streamlit (UI)\n",
        "\n",
        "W Streamlit zrobimy:\n",
        "- upload dokument√≥w,\n",
        "- indeksowanie do ChromaDB,\n",
        "- chat RAG (retrieval + LLM),\n",
        "- wyb√≥r modelu.\n",
        "\n",
        "W Colab uruchomimy Streamlit i wystawimy go przez tunel Cloudflare.\n"
      ],
      "id": "BWpkyWaCo7mY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0X_ZuZPo7mY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037cf6ae-a0ff-4e27-cd7e-b284df9d40e3"
      },
      "source": [
        "app_code = r'''\n",
        "import os, re, pathlib, time\n",
        "import streamlit as st\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import litellm\n",
        "from pypdf import PdfReader\n",
        "\n",
        "PERSIST_DIR = \"./chroma_db\"\n",
        "COLLECTION_NAME = \"docs\"\n",
        "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "@st.cache_resource\n",
        "def get_embedder():\n",
        "    return SentenceTransformer(EMBED_MODEL_NAME)\n",
        "\n",
        "@st.cache_resource\n",
        "def get_collection():\n",
        "    client = chromadb.PersistentClient(path=PERSIST_DIR, settings=Settings(anonymized_telemetry=False))\n",
        "    return client.get_or_create_collection(name=COLLECTION_NAME)\n",
        "\n",
        "def read_pdf_bytes(file_bytes) -> str:\n",
        "    from io import BytesIO\n",
        "    reader = PdfReader(BytesIO(file_bytes))\n",
        "    pages = []\n",
        "    for p in reader.pages:\n",
        "        pages.append(p.extract_text() or \"\")\n",
        "    return \"\\n\".join(pages)\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 150):\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    chunks, i = [], 0\n",
        "    while i < len(text):\n",
        "        chunks.append(text[i:i+chunk_size])\n",
        "        i += max(1, chunk_size - overlap)\n",
        "    return chunks\n",
        "\n",
        "def index_files(files, chunk_size=1000, overlap=150):\n",
        "    embedder = get_embedder()\n",
        "    collection = get_collection()\n",
        "    ids, docs, metas = [], [], []\n",
        "    for f in files:\n",
        "        name = pathlib.Path(f.name).name\n",
        "        ext = pathlib.Path(name).suffix.lower()\n",
        "        if ext == \".pdf\":\n",
        "            text = read_pdf_bytes(f.getvalue())\n",
        "        else:\n",
        "            text = f.getvalue().decode(\"utf-8\", errors=\"ignore\")\n",
        "        chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)\n",
        "        for j,ch in enumerate(chunks):\n",
        "            ids.append(f\"{name}::chunk{j}\")\n",
        "            docs.append(ch)\n",
        "            metas.append({\"source\": name, \"chunk\": j})\n",
        "    if not docs:\n",
        "        return 0\n",
        "    embeds = embedder.encode(docs, batch_size=32, show_progress_bar=False).tolist()\n",
        "    collection.add(ids=ids, documents=docs, metadatas=metas, embeddings=embeds)\n",
        "    return len(docs)\n",
        "\n",
        "def retrieve(query: str, k: int = 5):\n",
        "    embedder = get_embedder()\n",
        "    collection = get_collection()\n",
        "    q_emb = embedder.encode([query]).tolist()\n",
        "    res = collection.query(query_embeddings=q_emb, n_results=k, include=[\"documents\",\"metadatas\",\"distances\"])\n",
        "    hits = []\n",
        "    for doc, meta, dist in zip(res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n",
        "        hits.append({\"text\": doc, \"meta\": meta, \"distance\": float(dist)})\n",
        "    return hits\n",
        "\n",
        "def rag_answer(question: str, k: int, model: str):\n",
        "    hits = retrieve(question, k=k)\n",
        "    context = \"\\n\\n\".join([f\"[{i+1}] ({h['meta']['source']}, chunk {h['meta']['chunk']})\\n{h['text']}\" for i,h in enumerate(hits)])\n",
        "    system = \"Jeste≈õ pomocnym asystentem. Odpowiadaj po polsku. Je≈õli brakuje danych w kontek≈õcie, powiedz wprost czego nie wiesz.\"\n",
        "    user = f\"\"\"Pytanie: {question}\n",
        "\n",
        "Kontekst (wybrane fragmenty dokument√≥w):\n",
        "{context}\n",
        "\n",
        "Instrukcja: Odpowiedz na pytanie wy≈ÇƒÖcznie na podstawie kontekstu. Je≈õli kontekst nie wystarcza, napisz jakie informacje sƒÖ brakujƒÖce.\n",
        "\"\"\"\n",
        "    resp = litellm.completion(\n",
        "        model=model,\n",
        "        messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    return resp[\"choices\"][0][\"message\"][\"content\"], hits\n",
        "\n",
        "st.set_page_config(page_title=\"RAG (ChromaDB + LiteLLM)\", layout=\"wide\")\n",
        "st.title(\"üìö Prosty RAG: ChromaDB + LiteLLM + Streamlit\")\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Ustawienia\")\n",
        "    model = st.text_input(\"Model (LiteLLM)\", value=\"gpt-4o-mini\")\n",
        "    k = st.slider(\"Top-k fragment√≥w\", 1, 10, 5)\n",
        "    chunk_size = st.number_input(\"chunk_size\", 300, 3000, 1000, step=100)\n",
        "    overlap = st.number_input(\"overlap\", 0, 800, 150, step=50)\n",
        "\n",
        "    st.markdown(\"### Klucze API (opcjonalnie)\")\n",
        "    st.caption(\"Mo≈ºesz wpisaƒá tu klucz, a aplikacja ustawi go w env na czas sesji.\")\n",
        "    openai_key = st.text_input(\"OPENAI_API_KEY\", type=\"password\")\n",
        "    anthropic_key = st.text_input(\"ANTHROPIC_API_KEY\", type=\"password\")\n",
        "    gemini_key = st.text_input(\"GEMINI_API_KEY\", type=\"password\")\n",
        "    openrouter_key = st.text_input(\"OPENROUTER_API_KEY\", type=\"password\")\n",
        "\n",
        "    if openai_key: os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "    if anthropic_key: os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_key\n",
        "    if gemini_key: os.environ[\"GEMINI_API_KEY\"] = gemini_key\n",
        "    if openrouter_key: os.environ[\"OPENROUTER_API_KEY\"] = openrouter_key\n",
        "\n",
        "st.subheader(\"1) Indeksowanie dokument√≥w\")\n",
        "up = st.file_uploader(\"Wgraj pliki (PDF/TXT/MD)\", type=[\"pdf\",\"txt\",\"md\"], accept_multiple_files=True)\n",
        "if st.button(\"üì• Indeksuj\"):\n",
        "    if not up:\n",
        "        st.warning(\"Wgraj co najmniej 1 plik.\")\n",
        "    else:\n",
        "        n = index_files(up, chunk_size=int(chunk_size), overlap=int(overlap))\n",
        "        st.success(f\"Zindeksowano chunk√≥w: {n}\")\n",
        "\n",
        "st.divider()\n",
        "\n",
        "st.subheader(\"2) Chat RAG\")\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "for m in st.session_state.messages:\n",
        "    with st.chat_message(m[\"role\"]):\n",
        "        st.markdown(m[\"content\"])\n",
        "\n",
        "prompt = st.chat_input(\"Zadaj pytanie do dokument√≥w‚Ä¶\")\n",
        "if prompt:\n",
        "    st.session_state.messages.append({\"role\":\"user\",\"content\":prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        try:\n",
        "            ans, hits = rag_answer(prompt, k=int(k), model=model)\n",
        "            st.markdown(ans)\n",
        "            with st.expander(\"üîé ≈πr√≥d≈Ça (retrieval)\"):\n",
        "                for i,h in enumerate(hits,1):\n",
        "                    st.write(f\"#{i} dist={h['distance']:.4f} source={h['meta']}\")\n",
        "                    st.write(h[\"text\"])\n",
        "            st.session_state.messages.append({\"role\":\"assistant\",\"content\":ans})\n",
        "        except Exception as e:\n",
        "            st.error(f\"B≈ÇƒÖd LLM: {e}\")\n",
        "            st.info(\"Je≈õli nie masz klucza API, testuj retrieval w notebooku albo ustaw klucz w panelu bocznym.\")\n",
        "'''\n",
        "with open(\"app_rag.py\",\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"Wrote app_rag.py\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote app_rag.py\n"
          ]
        }
      ],
      "id": "g0X_ZuZPo7mY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrKoxeouo7mZ"
      },
      "source": [
        "### Uruchom Streamlit + tunel (Cloudflare)\n",
        "\n",
        "Po uruchomieniu kom√≥rki dostaniesz publiczny link (HTTPS). Otw√≥rz go w przeglƒÖdarce.\n"
      ],
      "id": "rrKoxeouo7mZ"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 /usr/local/bin/cloudflared\n"
      ],
      "metadata": {
        "id": "na4Z6P0CUDwF"
      },
      "id": "na4Z6P0CUDwF",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBMBBRl6o7mZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70624040-d49c-4c7b-d22e-804955dad6b4"
      },
      "source": [
        "# Uruchamiamy Streamlit w tle i wystawiamy przez Cloudflare Tunnel\n",
        "import subprocess, textwrap, time, os, signal, sys\n",
        "\n",
        "# Kill previous if rerun\n",
        "!pkill -f \"streamlit run app_rag.py\" || true\n",
        "!pkill -f \"cloudflared tunnel\" || true\n",
        "\n",
        "streamlit_proc = subprocess.Popen([\"streamlit\", \"run\", \"app_rag.py\",\n",
        "                                   \"--server.port\", \"8501\",\n",
        "                                   \"--server.enableCORS\", \"false\",\n",
        "                                   \"--server.enableXsrfProtection\", \"false\"],\n",
        "                                  stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "time.sleep(2)\n",
        "\n",
        "tunnel_proc = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8501\"],\n",
        "                               stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "# WyciƒÖgamy link z outputu tunelu\n",
        "public_url = None\n",
        "t0 = time.time()\n",
        "while time.time() - t0 < 20:\n",
        "    line = tunnel_proc.stdout.readline().strip()\n",
        "    if line:\n",
        "        print(line)\n",
        "    m = re.search(r\"(https://[\\w\\-\\.]+\\.trycloudflare\\.com)\", line)\n",
        "    if m:\n",
        "        public_url = m.group(1)\n",
        "        break\n",
        "\n",
        "print(\"\\n‚úÖ Publiczny link:\", public_url if public_url else \"(nie uda≈Ço siƒô odczytaƒá - sprawd≈∫ log powy≈ºej)\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "^C\n",
            "2026-01-21T11:37:59Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2026-01-21T11:37:59Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2026-01-21T11:38:03Z INF +--------------------------------------------------------------------------------------------+\n",
            "2026-01-21T11:38:03Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2026-01-21T11:38:03Z INF |  https://sustainability-enb-demands-willing.trycloudflare.com                              |\n",
            "\n",
            "‚úÖ Publiczny link: https://sustainability-enb-demands-willing.trycloudflare.com\n"
          ]
        }
      ],
      "id": "CBMBBRl6o7mZ"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8_lLmm2TWw_"
      },
      "id": "J8_lLmm2TWw_",
      "execution_count": null,
      "outputs": []
    }
  ]
}